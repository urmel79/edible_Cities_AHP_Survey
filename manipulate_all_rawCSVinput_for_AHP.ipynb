{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4266f6-c35a-45ba-99cf-10731fdbad32",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Why we use a [Jupyter](https://en.wikipedia.org/wiki/Project_Jupyter) notebook to to publish the R program examples:\n",
    "\n",
    "Jupyter is a new **open source** alternative to the proprietary numerical software [Mathematica](https://en.wikipedia.org/wiki/Wolfram_Mathematica) from **Wolfram Research** that is well on the way to become a **standard for exchanging research results** (<cite data-cite=\"Scientific_Paper_obsolete_2018\">Somers, 2018</cite>; <cite data-cite=\"Future_of_Research_Paper_2018\">Romer, 2018</cite>).\n",
    "\n",
    "Originally Jupyter was intended as an IDE for the programming languages **Julia** and **Python**. Besides that it is also possible to install other interpreter kernels, such as the **[IRkernel](https://irkernel.github.io/installation/)** for R. This can be interesting if the IDE **RStudio Desktop** is not available on the target platform used. For example, it is very difficult to install RStudio on the ARM-based embedded computer **Raspberry Pi** due to many technical dependencies. In contrast, using the R kernel in JupyterLab on the Raspberry Pi works very well and performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685263b8-8a36-40f2-973d-401e5ce0116b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings and dependencies\n",
    "\n",
    "## Install missing packages if not present yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0dd6c0-217b-4a72-af09-a86b2fabf834",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"All required packages are installed.\"\n"
     ]
    }
   ],
   "source": [
    "# List of R packages that are used in this script\n",
    "list.of.packages <- c(\"data.table\")\n",
    "\n",
    "# Query the already installed packages and save the missing ones in a new list\n",
    "missing.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "\n",
    "# Install missing packages\n",
    "if(length(missing.packages)) {\n",
    "    install.packages(missing.packages)\n",
    "} else {\n",
    "    print(\"All required packages are installed.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11b1cb-3708-4fe7-b80a-07ff02070a79",
   "metadata": {},
   "source": [
    "## Load package `data.table`\n",
    "\n",
    "The package `data.table` is used for reading and manipulating tables (`data.table` inherits from `data.frame`). Install and load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b50219-59aa-4292-be89-1f4cfc97e11e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(data.table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5a3fcc5",
   "metadata": {},
   "source": [
    "## Load packages `knitr` and `IRdisplay`\n",
    "\n",
    "The `kable()` function from the package `knitr` is used to output dataframes as a markdown tables.\n",
    "\n",
    "The `display_markdown()` function from the package `IRdisplay` renders the markdown table in the notebook as well as in the PDF version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53776e5a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(knitr)\n",
    "library(IRdisplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e090cf2-d697-4f66-9f9b-9806406fdeb8",
   "metadata": {},
   "source": [
    "## Set globally used input and output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2383fc4e-8a5a-4834-8216-e269151185a6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "str_input_path = \"./input_data_from_survey\"\n",
    "str_output_path = \"./output_data_manipulated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace923ec-141f-4537-923d-ed6c18ad699f",
   "metadata": {},
   "source": [
    "# Functions to prepare the survey data for further analysis\n",
    "\n",
    "The following functions are used to read the survey data from the CSV files and prepare the data structure for further analysis with the R package `ahpsurvey`.\n",
    "\n",
    "## Function to read the survey data from CSV files to dataframe objects\n",
    "\n",
    "Define a function for reading in a CSV file to 4 different dateframes by selecting different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925879d2-aa01-483e-97de-fd9ff863e472",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "func_readCSVdata_to_dataframes <- function(str_CSVfilename) {\n",
    "  \n",
    "  # criteria (main criteria)\n",
    "  df_mySurvey_1 <- fread(\n",
    "    file = str_CSVfilename, encoding = \"UTF-8\",\n",
    "    header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
    "    # dec = \".\", row.var = \"CASE\",\n",
    "    select = c(\"CASE\", \"AK01\", \"AK02\", \"AK03\", \n",
    "               \"RK01_01\", \"RK02_01\", \"RK03_01\", \"RK04_01\", \"RK05_01\", \"RK06_01\")\n",
    "    )\n",
    "  \n",
    "  # environmental sub-criteria\n",
    "  df_mySurvey_2 <- fread(\n",
    "    file = str_CSVfilename, encoding = \"UTF-8\",\n",
    "    header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
    "    # dec = \".\", row.names = \"CASE\",\n",
    "    select = c(\"CASE\", \"AU01\", \"AU02\", \"AU03\", \n",
    "               \"RU01_01\", \"RU02_01\", \"RU03_01\", \"RU04_01\", \"RU05_01\", \"RU06_01\")\n",
    "    )\n",
    "  \n",
    "  # social sub-criteria\n",
    "  df_mySurvey_3 <- fread(\n",
    "    file = str_CSVfilename, encoding = \"UTF-8\",\n",
    "    header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
    "    # dec = \".\", row.names = \"CASE\",\n",
    "    select = c(\"CASE\", \"AS01\", \"AS02\", \"AS03\", \n",
    "               \"RS01_01\", \"RS02_01\", \"RS03_01\", \"RS04_01\", \"RS05_01\", \"RS06_01\")\n",
    "    )\n",
    "  \n",
    "  # economic sub-criteria\n",
    "  df_mySurvey_4 <- fread(\n",
    "    file = str_CSVfilename, encoding = \"UTF-8\",\n",
    "    header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n",
    "    # dec = \".\", row.names = \"CASE\",\n",
    "    select = c(\"CASE\", \"AW01\", \"AW02\", \"AW03\", \n",
    "               \"RW01_01\", \"RW02_01\", \"RW03_01\", \"RW04_01\", \"RW05_01\", \"RW06_01\")\n",
    "    )\n",
    "  \n",
    "  output <- list(df_mySurvey_1, df_mySurvey_2, df_mySurvey_3, df_mySurvey_4)\n",
    "  \n",
    "  return(output)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d221b3ab",
   "metadata": {},
   "source": [
    "## Function to format dataframes as a markdown tables\n",
    "\n",
    "Following function formats given dataframes as markdown tables using the `kable()` function from the `knitr` package.\n",
    "\n",
    "The `display_markdown()` function from the package `IRdisplay` renders the markdown table in the notebook as well as in the PDF version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e556c038",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "func_render_md_tables <- function(df_table, str_table_header) {\n",
    "    # format the dataframe as a markdown table using the 'kable()' function from the 'knitr' package\n",
    "    table_out <- kable(\n",
    "        df_table,\n",
    "        format = \"markdown\",\n",
    "        # digits = 2,\n",
    "        caption = str_table_header)\n",
    "\n",
    "    display_markdown(as.character(table_out))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174d878-6a9e-4d8c-92d7-8b64c2e3d7c7",
   "metadata": {},
   "source": [
    "## Function to prepare the data and store it in new dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0045f26b-73f7-4d1a-94f5-40bc42f84a65",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "func_scrambleData <- function(df_inputData, vec_colnames_search_1, vec_colnames_search_2, vec_colnames_out) {\n",
    "  # Generate new data frame ...\n",
    "  df_outputData <- data.frame(matrix(ncol = 3, nrow = 0))\n",
    "  # ... and name the columns\n",
    "  colnames(df_outputData) <- vec_colnames_out\n",
    "  \n",
    "  # Generate 1. column\n",
    "  for ( row_idx in 1:nrow(df_inputData) ) {\n",
    "    # filter column names by vector element\n",
    "    if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[1], with=FALSE] == 1) {\n",
    "      int_tmp_val <- as.integer(df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_2[1], with=FALSE])\n",
    "      int_tmp_val <- int_tmp_val * -1 - 1\n",
    "\n",
    "      df_outputData[row_idx, vec_colnames_out[1]] <- int_tmp_val\n",
    "    }\n",
    "    else if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[1], with=FALSE] == -1) {\n",
    "      df_outputData[row_idx, vec_colnames_out[1]] <- 1\n",
    "    }\n",
    "    else if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[1], with=FALSE] == 2) {\n",
    "      int_tmp_val <- as.integer(df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_2[2], with=FALSE])\n",
    "      int_tmp_val <- int_tmp_val + 1\n",
    "\n",
    "      df_outputData[row_idx, vec_colnames_out[1]] <- int_tmp_val\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Generate 2. column\n",
    "  for ( row_idx in 1:nrow(df_inputData) ) {\n",
    "    # filter column names by vector element\n",
    "    if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[2], with=FALSE] == 1) {\n",
    "      int_tmp_val <- as.integer(df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_2[3], with=FALSE])\n",
    "      int_tmp_val <- int_tmp_val * -1 - 1\n",
    "      \n",
    "      df_outputData[row_idx, vec_colnames_out[2]] <- int_tmp_val\n",
    "    } \n",
    "    else if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[2], with=FALSE] == -1) {\n",
    "      df_outputData[row_idx, vec_colnames_out[2]] <- 1\n",
    "    } \n",
    "    else if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[2], with=FALSE] == 2) {\n",
    "      int_tmp_val <- as.integer(df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_2[4], with=FALSE])\n",
    "      int_tmp_val <- int_tmp_val + 1\n",
    "      \n",
    "      df_outputData[row_idx, vec_colnames_out[2]] <- int_tmp_val\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Generate 3. column\n",
    "  for ( row_idx in 1:nrow(df_inputData) ) {\n",
    "    # filter column names by vector element\n",
    "    if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[3], with=FALSE] == 1) {\n",
    "      int_tmp_val <- as.integer(df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_2[5], with=FALSE])\n",
    "      int_tmp_val <- int_tmp_val * -1 - 1\n",
    "      \n",
    "      df_outputData[row_idx, vec_colnames_out[3]] <- int_tmp_val\n",
    "    } \n",
    "    else if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[3], with=FALSE] == -1) {\n",
    "      df_outputData[row_idx, vec_colnames_out[3]] <- 1\n",
    "    } \n",
    "    else if (df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_1[3], with=FALSE] == 2) {\n",
    "      int_tmp_val <- as.integer(df_inputData[row_idx, colnames(df_inputData) %in% vec_colnames_search_2[6], with=FALSE])\n",
    "      int_tmp_val <- int_tmp_val + 1\n",
    "      \n",
    "      df_outputData[row_idx, vec_colnames_out[3]] <- int_tmp_val\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # return scrambled data frame\n",
    "  return(df_outputData)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288d2bf-a084-4843-9d3e-5b841f5d4c11",
   "metadata": {},
   "source": [
    "## Function to write resulting dataframes to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da629582-2ad9-4c40-8041-961c3d9859b2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "func_writeDataframe_to_CSVfile <- function(str_path, str_CSVfilename, df_dataframe, str_filenameExtension) {\n",
    "  # Split file name on second underscore, found here: \n",
    "  # https://stackoverflow.com/questions/32398427/r-split-a-character-string-on-the-second-underscore/32398489#32398489\n",
    "  list_str_split <- strsplit(sub('(^[^_]+_[^_]+)_(.*)$', '\\\\1 \\\\2', str_CSVfilename), ' ')\n",
    "  \n",
    "  # extend the file name prefix and glue together with old suffix\n",
    "  str_CSVfilename_extended <- paste(list_str_split[[1]][1], str_filenameExtension, list_str_split[[1]][2], sep=\"_\")\n",
    "  \n",
    "  # extend file name by path\n",
    "  str_CSVfilename_extended <- paste(str_path, str_CSVfilename_extended, sep=\"/\")\n",
    "  \n",
    "  write.table(df_dataframe, file = str_CSVfilename_extended,\n",
    "              fileEncoding = \"UTF-8\", row.names = FALSE,\n",
    "              col.names = TRUE, sep = \"\\t\", quote = TRUE)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "657abd23-18f5-41bb-baf4-e584efb2d0e3",
   "metadata": {},
   "source": [
    "# Create data frame (table) handling the file names of input CSV data (raw data from survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513c1ac2-6979-4726-a9f3-eab1e1e5ff4b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Table: File table for handling the file names of input CSV data (raw data from survey)\n",
       "\n",
       "| file_idx|keys |filenames                                        |descriptions                   |\n",
       "|--------:|:----|:------------------------------------------------|:------------------------------|\n",
       "|        1|all  |rdata_all_AHP_edible_Cities_2022-03-18_09-53.csv |all target groups together     |\n",
       "|        2|CA   |rdata_CA_AHP_edible_Cities_2022-03-18_10-28.csv  |City Administrations           |\n",
       "|        3|NGO  |rdata_NGO_AHP_edible_Cities_2022-03-18_10-40.csv |Non-Governmental Organisations |\n",
       "|        4|PE   |rdata_PE_AHP_edible_Cities_2022-03-18_10-41.csv  |Practitioners and Experts      |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_csvInputFiles <- data.table(\n",
    "  file_idx = 1:4,\n",
    "  keys = c(\"all\", \"CA\", \"NGO\", \"PE\"),\n",
    "  filenames = c(\"rdata_all_AHP_edible_Cities_2022-03-18_09-53.csv\",\n",
    "                \"rdata_CA_AHP_edible_Cities_2022-03-18_10-28.csv\",\n",
    "                \"rdata_NGO_AHP_edible_Cities_2022-03-18_10-40.csv\",\n",
    "                \"rdata_PE_AHP_edible_Cities_2022-03-18_10-41.csv\"),\n",
    "  descriptions = c(\"all target groups together\",\n",
    "                   \"City Administrations\",\n",
    "                   \"Non-Governmental Organisations\",\n",
    "                   \"Practitioners and Experts\")\n",
    ")\n",
    "\n",
    "func_render_md_tables(df_csvInputFiles, \"File table for handling the file names of input CSV data (raw data from survey)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e8fe50e-af73-4659-931b-d9ac7f34ac02",
   "metadata": {},
   "source": [
    "# Prepare the data and store it in new CSV files for each criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09307342-dbf7-4019-ad47-b1ddd0a99e28",
   "metadata": {},
   "source": [
    "## Criteria (main criteria)\n",
    "\n",
    "Walk over all input CSV files, select necessary columns, filter cells by given algorithm, and write the results to output CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd50fb1-5320-472f-b43f-207d94a1f6be",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "vec_colnames_search_1 <- c('AK01', 'AK02', 'AK03')\n",
    "vec_colnames_search_2 <- c('RK01_01', 'RK02_01', 'RK03_01', 'RK04_01', 'RK05_01', 'RK06_01')\n",
    "vec_colnames_out <- c('Envi_Soci', 'Envi_Econ', 'Soci_Econ')\n",
    "\n",
    "for ( row_idx in 1:nrow(df_csvInputFiles) ) {\n",
    "  # create list of data frames from current input CSV file\n",
    "  str_filename <- paste(str_input_path, df_csvInputFiles[row_idx, filenames], sep=\"/\")\n",
    "  list_dataframes <- func_readCSVdata_to_dataframes(str_filename)\n",
    "  \n",
    "  # scramble the data frames\n",
    "  df_scrambledData <- func_scrambleData(list_dataframes[[1]], vec_colnames_search_1, vec_colnames_search_2, vec_colnames_out)\n",
    "  \n",
    "  # write scrambled data frames to output CSV file\n",
    "  func_writeDataframe_to_CSVfile(str_output_path, df_csvInputFiles[row_idx, filenames], df_scrambledData, \"crit\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d06d2f",
   "metadata": {},
   "source": [
    "## Environmental sub-criteria\n",
    "\n",
    "Walk over all input CSV files, select necessary columns, filter cells by given algorithm, and write the results to output CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a585b55c-41fc-4b9d-be31-bd4f2efdf85a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "vec_colnames_search_1 <- c('AU01', 'AU02', 'AU03')\n",
    "vec_colnames_search_2 <- c('RU01_01', 'RU02_01', 'RU03_01', 'RU04_01', 'RU05_01', 'RU06_01')\n",
    "vec_colnames_out <- c('Clim_BDiv', 'Clim_CiEc', 'BDiv_CiEc')\n",
    "\n",
    "for ( row_idx in 1:nrow(df_csvInputFiles) ) {\n",
    "  # create list of data frames from current input CSV file\n",
    "  str_filename <- paste(str_input_path, df_csvInputFiles[row_idx, filenames], sep=\"/\")\n",
    "  list_dataframes <- func_readCSVdata_to_dataframes(str_filename)\n",
    "  \n",
    "  # scramble the data frames\n",
    "  df_scrambledData <- func_scrambleData(list_dataframes[[2]], vec_colnames_search_1, vec_colnames_search_2, vec_colnames_out)\n",
    "  \n",
    "  # write scrambled data frames to output CSV file\n",
    "  func_writeDataframe_to_CSVfile(str_output_path, df_csvInputFiles[row_idx, filenames], df_scrambledData, \"env\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651dc078-6c3a-4268-99b1-183b01d9b6ca",
   "metadata": {},
   "source": [
    "## Social sub-criteria\n",
    "\n",
    "Walk over all input CSV files, select necessary columns, filter cells by given algorithm, and write the results to output CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d64049a-9042-4869-8c6c-043435726769",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "vec_colnames_search_1 <- c('AS01', 'AS02', 'AS03')\n",
    "vec_colnames_search_2 <- c('RS01_01', 'RS02_01', 'RS03_01', 'RS04_01', 'RS05_01', 'RS06_01')\n",
    "vec_colnames_out <- c('KEdu_Comm', 'KEdu_Part', 'Comm_Part')\n",
    "\n",
    "for ( row_idx in 1:nrow(df_csvInputFiles) ) {\n",
    "  # create list of data frames from current input CSV file\n",
    "  str_filename <- paste(str_input_path, df_csvInputFiles[row_idx, filenames], sep=\"/\")\n",
    "  list_dataframes <- func_readCSVdata_to_dataframes(str_filename)\n",
    "  \n",
    "  # scramble the data frames\n",
    "  df_scrambledData <- func_scrambleData(list_dataframes[[3]], vec_colnames_search_1, vec_colnames_search_2, vec_colnames_out)\n",
    "  \n",
    "  # write scrambled data frames to output CSV file\n",
    "  func_writeDataframe_to_CSVfile(str_output_path, df_csvInputFiles[row_idx, filenames], df_scrambledData, \"soc\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b42ec-288b-4b4b-8fef-0fbcd7ecb118",
   "metadata": {},
   "source": [
    "## Economic sub-criteria\n",
    "\n",
    "Walk over all input CSV files, select necessary columns, filter cells by given algorithm, and write the results to output CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602c734a-f4aa-47b7-b706-278b1e39f7ba",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "vec_colnames_search_1 <- c('AW01', 'AW02', 'AW03')\n",
    "vec_colnames_search_2 <- c('RW01_01', 'RW02_01', 'RW03_01', 'RW04_01', 'RW05_01', 'RW06_01')\n",
    "vec_colnames_out <- c('Qual_LVCs', 'Qual_Affo', 'LVCs_Affo')\n",
    "\n",
    "for ( row_idx in 1:nrow(df_csvInputFiles) ) {\n",
    "  # create list of data frames from current input CSV file\n",
    "  str_filename <- paste(str_input_path, df_csvInputFiles[row_idx, filenames], sep=\"/\")\n",
    "  list_dataframes <- func_readCSVdata_to_dataframes(str_filename)\n",
    "  \n",
    "  # scramble the data frames\n",
    "  df_scrambledData <- func_scrambleData(list_dataframes[[4]], vec_colnames_search_1, vec_colnames_search_2, vec_colnames_out)\n",
    "  \n",
    "  # write scrambled data frames to output CSV file\n",
    "  func_writeDataframe_to_CSVfile(str_output_path, df_csvInputFiles[row_idx, filenames], df_scrambledData, \"eco\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b16c03-c6f7-4fa8-bc94-26f2529aa977",
   "metadata": {},
   "source": [
    "# Summary and outlook"
   ]
  }
 ],
 "metadata": {
  "abstract": "This is a placeholder for the abstract that needs to be added later.",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "latex_metadata": {
   "affiliation1": "Berufsgenossenschaft Energie Textil Elektro Medienerzeugnisse",
   "affiliation2": "Leibniz Institute of Ecological Urban and Regional Development",
   "author1": "Bj\\\"orn Kasper",
   "author2": "Henriette John",
   "bib": "literature/notebook.bib",
   "cover_image": "images/Cover_image.pdf",
   "email1": "kasper.bjoern@bgetem.de",
   "email2": "h.john@ioer.de",
   "release_version": "version 0.1 (pre-release)",
   "title": "Preparing raw CSV input data from survey for analytical hierarchy process (AHP)"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
